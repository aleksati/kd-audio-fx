# Lightweight Neural Audio Distortion FX With Knowledge Distillation

This code repository contains scripts, code and models for the article _Compressing Neural Network Models of Audio Distortion Effects Using Knowledge Distillation Techniques_, accepted at ... conference in ... 2025.

Knowledge distillation is a technique for compressing complex and large "teacher" networks into smaller "student" networks. It offers ways to minimize the computational expenses often associated with neural networks and to optimize models for deployment and real-time usage. In our paper, we explore the application of knowledge distillation for compressing RNN models of audio distortion effects. In particular, we propose an audio-to-audio LSTM architecture for realtime regression tasks where small audio effect networks are trained to mimic the internal representations of more extensive networks, known as feature-based knowledge distillation.

<div align="left">
 <img src="./fig/dk2.png" width="400">
</div>

Our distillation architecture was evaluated on three datasets, the Blackstar HT-1 vacuum tube amplifier (HT-1), Electro-Harmonix Big Muff (Big Muff) guitar pedal, and the analog-modeled overdrive plugin DrDrive.

 
# Audio Examples

Visit our designated [GitHub page for audio examples]().

<!-- Our distillation architecture was evaluated on three datasets, the Blackstar HT-1 vacuum tube amplifier (HT-1), Electro-Harmonix Big Muff (Big Muff) guitar pedal, and the analog-modeled overdrive plugin DrDrive.

Below are just a few non-parametric examples comparing our distilled student models against regular students networks (non-distilled).

**64 unit networks**
DrDrive - Target - Distilled - NonDistilled
HT-1 - Target - Distilled - NonDistilled

**8 unit networks**
Big Muff - Target - Distilled - NonDistilled -->

# Usage

This repository contains all the necessary utilities to use our knowledge distillation architecture. Find the code and pre-trained models located inside the "./src" folder.

## Datasets
Our distillation architecture was evaluated on three datasets, the Blackstar HT-1 vacuum tube amplifier (HT-1), Electro-Harmonix Big Muff (Big Muff) guitar pedal, and the analog-modeled overdrive plugin DrDrive.

The datasets of DrDrive (both parametric and non-parametric) were created for this article and can be accessed via [Zenodo](https://doi.org/10.5281/zenodo.15222630).

The HT-1 and Big Muff datasets were sourced from the IEEE journal article [Pruning Deep Neural Network Models of Guitar Distortion Effects](https://ieeexplore.ieee.org/abstract/document/9954902/), by David Südholt, Alec Wright, Cumhur Erkut and Vesa Valimaki, 2023.

## Models

A selection of pre-trained teacher and student models can be found in the "./src/models" folder. Although both conditioned and unconditioned versions are listed, conditioned models exist only for DrDrive.

Student models are either *non_distilled*, trained on the normal training data + test sets, or *distilled*, trained on transfer data generated by a teacher model + the normal test sets. Further, every student is constructured with only one LSTM layer of 8, 16 32 or 64 units.

The folder structure:
```
./src
├── code
├── models
│   ├── conditioned       
│   └── unconditioned
│       ├── students_distilled
│       │   └── LSTM_DEVICE_UNITS (f.ex "LSTM_bigmuff_16")
│       ├── students_distilled_by_64_student
│       ├── students_non_distilled
│       └── teachers 
```

Finally, one experimental model is included in the "./unconditioned/student_distilled_by_64_student" folder. In this case, smaller student models (8 units) have been trained by a larger student model (64 units), instead of teachers.

## How To Run 


First, install Python dependencies:
```
cd ./src/code
pip install -r requirements.txt
```

Next, choose an environment to train, like teacher, distilled student, non-distilled student, etc. 

Every subfolder inside ".src/code/" contains its own starter script for running the code from a terminal. The code folder structure:
```
./src
├── code    
│   ├── requirements.txt
│   ├── conditioned
│   └── unconditioned  
│       ├── teacher
│       │   └── starter.py
│       ├── create_dataset_from_teacher 
│       ├── student_distilled 
│       └── student_non_distilled
└── models
``` 

Inside every subfolder in the code folder there exists a unique starter script. 

Creater starter for training the model with terminal
Same for training and inference (flag)

A starter per folder.
one teacher. not gridsearch

Another starter for conditioning. choose between conditioning and not.

https://github.com/Alec-Wright/Automated-GuitarAmpModelling/blob/main/proc_audio.py


```
cd ./src/code/teacher
python starter.py < OPTIONS >
```

Avaliable options: 
* SAVE_DIR (str) - path to directory to save the model and other results (default="../models")
* DATA_DIR (str) - path to directory where the datasets are stored (default="../datasets")
* DATASETS (array of strings) - name of the dataset to be used (default=["DrDrive", "ht1", "bigmuff"])
* EPOCHS (int) - number of epochs for training (defaut=1000)
* PARAMETER_NUMBER (int) - number of parameters. Only for parametric training (default=0)
* BATCH_SIZE (int) - the size of each batch (default=8) 
* MINI_BATCH_SIZE (int) - only for teacher. ?
* LR (int) - the initial learning rate (default=3e-4)
* UNITS (array of int) - the number of units in each LSTM layer (default=various)

* INFERENCE (bool) - flag to state whether or not to run inference on the model after training? Or just inference with no training? 

Example: 
```
cd ./src/code/teacher
python ....
```

# VST Download

Avaliable soon.

aleks try with neutone
ricc with other.
