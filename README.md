# Lightweight Neural Audio Distortion FX Using Knowledge Distillation

This code repository contains scripts, code and models for the article _Compressing Neural Network Models of Audio Distortion Effects Using Knowledge Distillation Techniques_, accepted at ... conference in ... 2025.

Knowledge distillation is a technique for compressing complex and large "teacher" networks into smaller "student" networks. It offers ways to minimize the computational expenses often associated with neural networks and to optimize models for deployment and real-time usage. In our paper, we explore the application of knowledge distillation for compressing RNN models of audio distortion effects. In particular, we propose an audio-to-audio LSTM architecture for realtime regression tasks where small audio effect networks are trained to mimic the internal representations of more extensive networks, known as feature-based knowledge distillation.

<div align="left">
 <img src="./fig/dk2.png" width="400">
</div>

Visit our designated [GitHub page for audio examples]().

<!-- Our distillation architecture was evaluated on three datasets, the Blackstar HT-1 vacuum tube amplifier (HT-1), Electro-Harmonix Big Muff (Big Muff) guitar pedal, and the analog-modeled overdrive plugin DrDrive.

Below are just a few non-parametric examples comparing our distilled student models against regular students networks (non-distilled).

**64 unit networks**
DrDrive - Target - Distilled - NonDistilled
HT-1 - Target - Distilled - NonDistilled

**8 unit networks**
Big Muff - Target - Distilled - NonDistilled -->

# Usage

This repository contains all the necessary utilities to use our knowledge distillation architecture. Find the code and pre-trained models located inside the "./src" folder.

## Datasets

Our distillation architecture was evaluated on three datasets, the Blackstar HT-1 vacuum tube amplifier (HT-1), Electro-Harmonix Big Muff (Big Muff) guitar pedal, and the analog-modeled overdrive plugin DrDrive.

The datasets of DrDrive (both parametric and non-parametric) created for this article by us, can be accessed via [Zenodo](https://doi.org/10.5281/zenodo.15222630). The HT-1 and Big Muff datasets were sourced from the IEEE journal article [Pruning Deep Neural Network Models of Guitar Distortion Effects](https://ieeexplore.ieee.org/abstract/document/9954902/), by David Südholt, Alec Wright, Cumhur Erkut and Vesa Valimaki, 2023.

Put all datasets in the datasets folder using the .pickle extenstion and the following naming convention:
```
./src
├── code
├── datasets
│   ├── drdrive_test.pickle
│   └── drdrive_train.pickle
├── models
```

## Models

A selection of pre-trained teacher and student models can be found in the "./src/models" folder. Although both conditioned and unconditioned versions are listed, conditioned models exist only for DrDrive.

Student models are either *non_distilled*, trained on the normal training data + test sets, or *distilled*, trained on transfer data generated by a teacher model + the normal test sets. Further, students are constructed with two LSTM layers in addition to their input and output dense layers. The size of the first LSTM layer is customizable through user arguments (see "how to run") while the second layer is always 8 units.

The folder structure:
```
./src
├── code
├── datasets
├── models
│   ├── conditioned       
│   └── unconditioned
│       ├── students_distilled
│       │   └── LSTM_DEVICE_UNITS (f.ex "LSTM_bigmuff_16")
│       ├── students_distilled_by_64_student
│       ├── students_non_distilled
│       └── teachers 
```

Finally, one experimental model is included in the "./unconditioned/student_distilled_by_64_student" folder. In this case, smaller student models (8 units) have been trained by a larger student model (64 units), instead of teachers.

## How To Run 

First, install Python dependencies:
```
cd ./src/code
pip install -r requirements.txt
```

Next, choose whether to train a teacher, distilled student or non-distilled student network. 

Order of operatations:
1. Load datasets into the datasets folder
2. Train teacher model
3. Create transfer dataset from teacher model (create_transferset_from_teacher)
4. Train non-distilled and distilled student models. 

Every subfolder inside ".src/code/" contains its own starter script for running the code from a terminal. The code folder structure:

```
./src
├── code    
│   ├── requirements.txt
│   ├── conditioned
│   └── unconditioned  
│       ├── teacher
│       │   └── starter.py
│       ├── create_transferset_from_teacher 
│       ├── student_distilled 
│       └── student_non_distilled
├── datasets
└── models
``` 

Inside every subfolder in the code folder there exists a unique starter script. 

Creater starter for training the model with terminal
Same for training and inference (flag)

A starter per folder.
one teacher. not gridsearch

Another starter for conditioning. choose between conditioning and not.

https://github.com/Alec-Wright/Automated-GuitarAmpModelling/blob/main/proc_audio.py

Will use the default arguments.

fetch from datasets folder and store data in the models folder

```
cd ./src/code/unconditioned/teacher
python starter.py [options ...]
```

Avaliable options: 
* --datasets (array of str) - The names of the datasets to use, for example "--datatsets drdrive ht1". (default="drdrive") 
* --model_save_dir (str) - Folder directory in which to store the model and all other results (loss). (default ="./src/models")
* --data_dir (str) - Folder directory in which the datasets are stored (default="./src/datasets")
* --epochs (int) - Number of training epochs. (defaut=60)
* --parameter_number (int) - Number of conditioning parameters (default=0)
* --batch_size (int) - The size of each batch (default=8) 
* --hidden_layer_sizes (array of int) = The hidden layer size (amount of units) of the LSTM network. To train multiple networks with different hidden layer sizes, simply select more than one value. for instance, "--hidden_layer_size 8 16 32" will train three seperate networks, each with different units. Only for students (default=[8])
* --mini_batch_size (int) - The mini batch size (default=2048) 
* --learning_rate (float) - the initial learning rate (default=3e-4)
* --only_inference (bool) - When True, skips training and runs only inference on the pre-model. When False, runs training and inference on the trained model. (default=False)

Example: 
```
cd ./src/code/teacher
python ....
``` 

# VST Download

Avaliable soon.

aleks try with neutone
ricc with other.
